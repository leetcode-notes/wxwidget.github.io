---
layout: post
title: "机器学习的通用指南: 优化理论"
date: 2014-05-26 11:18
comments: true
math: true
categories: "Machine&nbspLearning Numeric&nbspOptimization"
abstract: 机器学习本质上是一种最优化问题，本文试图通过介绍求解最优化问题中最常见的梯度法，以窥见机器学习的通用学习策略。
---

通常我们会把机器学习过程分成两个阶段，建模阶段和求解阶段。在大数据的推动下，求解的问题慢慢大于建模的问题。比如，业界一般采用逻辑回归做CTR预估或者其他的预测模型，但是在数据规模非常大的条件下，如何在分布式的环境下，更快更准的求解模型就变得非常重要。精确的解析求解场景是非常少的，人们更多的去研究模型的数值求解过程，比如梯度下降法；近似求解，比如：蒙特卡洛法。这里我只想讨论下梯度下降法，该方法是一种非常神奇的通用的数学最优化方法。

下面会分几个阶段去讲梯度法的基本原理，本文更多的是偏理解。

1. 梯度下降法
2. 牛顿法
3. 拟牛顿法
4. 共轭梯度法

<!-- more -->

## 符号说明

- $x$表示自变量
- $y$表示因变量，是自变量的函数$y(x)$
- $\nabla$ 梯度算子
- $\eta$ 学习率，大于0的数
- g, $\nabla f$ 一阶梯度，
- H, $\nabla^2 f$ 二阶梯度

## 梯度法

简单的无约束的优化问题是，找到使得f(x)取最小值时的x。迭代法的基本思路是找到一种迭代的方法, x变化导致y下降,用数学的方法表示就是。

$$
x_{k+1} = x_k + p = x_k + \eta d
$$

其中p表示x的增加的部分，分解成大小$\eta$，和方向$d$

我们先得知道函数的近似公式：

<div class="definition">
    一阶泰勒展开公式: 
    $$f(x_k+p) = f(x_k)+ \nabla f(x_k) p + O(p^2)$$
    去掉无穷小部分
    $$f(x+p) = f(x)+ g^Tp  $$
</div>

从一阶的泰勒展开公式我们可以知道要想使得更新后的函数$f(x+p)$比$(x)$要小，必须使得 $g^T*d$小于等于0，我们又知道g和d都是向量, 我们可以
知道

$$
    g^Td = |g|*|d|*cos \theta
$$

只要保证\$theta$小于0，就可以保证 $g^Td$小于0。但是通常我们会用非常贪心的方法，使得$\theta$为最小值-1， 这就是传说中的最速梯度法了.

<div class="definition">
    梯度下降法: 
    $$p = -\eta * g,  满足 \eta \gt 0 $$
    那么x写成迭代形式是
    $$
    x_{k+1} = x_k - \eta * g
    $$
</div>


通过梯度下降法，我们知道，我们确实有一种方法，能找到x的一种迭代方式，使得函数y不增加。$x_{k+1} = x_k + \eta *g $中我们只解决了方向的问题，
还有一个学习率的问题没有解决。就是我们如何确定 $\eta$，有一种非常简单的方法是预设$\eta$为一个固定的值比如0.1；或者
$\eta$是在一个区间了下降，比如从0.5下降到0.001；或者不同的x的分量有不同的学习率，等等。这些方法通常表现都不错

## 牛顿法

为了得到更快的收敛速度，挑选合适的学习速率也是一件比较困难的事情。回到泰勒展开公式，我们知道还有一个更加精确的二阶泰勒展开公式

<div class="definition">
    二阶泰勒展开公式: 
    $$f(x_k+p) = f(x_k)+ \nabla f(x_k) p + \frac{1}{2} p^T \nabla ^2 f(x_k) p $$
    知道 $p = x - x_k$得到:
    $$f(x) = f(x_k)+ \nabla f(x_k) (x-x_k) + \frac{1}{2} (x-x_k)^T \nabla ^2 f(x_k) (x-x_k) $$
</div>


我们知道函数极值的条件是倒数为0,所以

$$ \frac{{\partial f}}{{\partial x}} = 0 $$

求解得到

$$ x = {x_k} - \frac{{\nabla f({x_k})}}{{{\nabla ^2}f({x_k})}} = {x_k} - {H^{ - 1}}g$$


以上我们知道
## line search

